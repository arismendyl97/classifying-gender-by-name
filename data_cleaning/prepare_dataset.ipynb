{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace accented vowels\n",
    "def remove_accents(text):\n",
    "    accents = 'áéíóúÁÉÍÓÚ'\n",
    "    replacements = 'aeiouAEIOU'\n",
    "    translation_table = str.maketrans(accents, replacements)\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3836/2104614329.py:12: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  source_2_1 = pd.read_csv('../dataset/source_2_1.csv',delimiter=';')\n"
     ]
    }
   ],
   "source": [
    "source_1_1 = pd.read_csv('../dataset/source_1_1.csv')\n",
    "source_1_1['excel'] = 'source_1_1'\n",
    "source_1_2 = pd.read_csv('../dataset/source_1_2.csv')\n",
    "source_1_2['excel'] = 'source_1_2'\n",
    "source_1_3 = pd.read_csv('../dataset/source_1_3.csv')\n",
    "source_1_3['excel'] = 'source_1_3'\n",
    "source_1_4 = pd.read_csv('../dataset/source_1_4.csv')\n",
    "source_1_4['excel'] = 'source_1_4'\n",
    "source_1_5 = pd.read_csv('../dataset/source_1_5.csv')\n",
    "source_1_5['excel'] = 'source_1_5'\n",
    "\n",
    "source_2_1 = pd.read_csv('../dataset/source_2_1.csv',delimiter=';')\n",
    "people_df = source_2_1[['nombre','sexo_id']].rename(columns={\"nombre\": \"name\", \"sexo_id\": \"gender\"})\n",
    "people_df[\"gender\"] = people_df[\"gender\"].apply(lambda x: 'F' if x == 1.0 else 'M' if x == 2.0 else '?')\n",
    "people_df = people_df[people_df[\"gender\"]!='?']\n",
    "people_df['code'] = 'ES'\n",
    "people_df['excel'] = 'people_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([source_1_1,source_1_2,source_1_3,source_1_4,source_1_5,people_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern to identify non-letter characters\n",
    "pattern = r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ ]'\n",
    "\n",
    "# Filter the DataFrame to keep only rows without non-letter characters, handling NaN values\n",
    "df = df[df['name'].notna()]\n",
    "\n",
    "df = df[\n",
    "    (df['code'].isin(['ES', 'CO', 'PE', 'CL'])) &\n",
    "    (df['name'] != '') &    # Ensure 'name' is not an empty string\n",
    "    (df['gender'].isin(['M', 'F'])) &    # Ensure 'name' is not an empty string\n",
    "    (~df['name'].str.contains(pattern))  # Apply regex pattern\n",
    "]\n",
    "\n",
    "df['gender'] = df['gender'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "df['name'] = df['name'].apply(lambda x: remove_accents(x))\n",
    "\n",
    "# Drop duplicates and reset index\n",
    "df['name'] = df['name'].str.lower()\n",
    "df = df[['name','gender']].drop_duplicates(subset=['name','gender']).reset_index(drop=True)\n",
    "df = df.sort_values(by=['gender', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 57491\n",
      "Validation set size: 8213\n",
      "Test set size: 16426\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "# Split the temporary set into validation (10%) and test set (20%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=2/3, random_state=seed)\n",
    "\n",
    "# Display the sizes of each set\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('spanish names db - training.csv', index=False)\n",
    "val_df.to_csv('spanish names db - validation.csv', index=False)\n",
    "test_df.to_csv('spanish names db - testing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pabuc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
